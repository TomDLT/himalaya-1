{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Refine multiple-kernel ridge results\nThis example demonstrates how to solve multiple-kernel ridge regression with\nhyperparameter random search, then refine the results with hyperparameter\ngradient descent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nfrom himalaya.backend import set_backend\nfrom himalaya.kernel_ridge import MultipleKernelRidgeCV\nfrom himalaya.kernel_ridge import Kernelizer\nfrom himalaya.kernel_ridge import ColumnKernelizer\nfrom himalaya.kernel_ridge import generate_dirichlet_samples\n\nfrom sklearn.pipeline import make_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, we use the ``cupy`` backend (GPU).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "backend = set_backend(\"cupy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can display the ``scikit-learn`` pipeline with an HTML diagram.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn import set_config\nset_config(display='diagram')  # requires scikit-learn 0.23"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate a random dataset\n- Xs_train : list of arrays of shape (n_samples_train, n_features)\n- Xs_test : list of arrays of shape (n_samples_test, n_features)\n- Y_train : array of shape (n_samples_train, n_targets)\n- Y_test : array of shape (n_repeat, n_samples_test, n_targets)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_kernels = 4\nn_targets = 500\n\n# We create a few kernel weights\nrng = np.random.RandomState(42)\nkernel_weights_true = generate_dirichlet_samples(n_targets, n_kernels,\n                                                 concentration=[.3],\n                                                 random_state=rng)\nkernel_weights_true = backend.to_numpy(kernel_weights_true)\n\n# Then, we generate a random dataset, using the arbitrary scalings.\nn_samples_train = 1000\nn_samples_test = 400\nn_features_list = np.full(n_kernels, fill_value=1000)\n\nXs_train, Xs_test = [], []\nY_train, Y_test = None, None\nfor ii in range(n_kernels):\n    n_features = n_features_list[ii]\n\n    X_train = rng.randn(n_samples_train, n_features)\n    X_test = rng.randn(n_samples_test, n_features)\n    X_train -= X_train.mean(0)\n    Xs_train.append(X_train)\n    Xs_test.append(X_test)\n\n    weights = rng.randn(n_features, n_targets) / n_features\n    weights *= kernel_weights_true[:, ii] ** 0.5\n\n    if ii == 0:\n        Y_train = X_train @ weights\n        Y_test = X_test @ weights\n    else:\n        Y_train += X_train @ weights\n        Y_test += X_test @ weights\n\nstd = Y_train.std(0)[None]\nY_train /= std\nY_test /= std\n\nnoise = 0.1\nY_train += rng.randn(n_samples_train, n_targets) * noise\nY_test += rng.randn(n_samples_test, n_targets) * noise\nY_test -= Y_test.mean(0)\nY_train -= Y_train.mean(0)\n\n# Concatenate the feature spaces.\nX_train = np.asarray(np.concatenate(Xs_train, 1), dtype=\"float32\")\nX_test = np.asarray(np.concatenate(Xs_test, 1), dtype=\"float32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare the pipeline\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Find the start and end of each feature space X in Xs\nstart_and_end = np.concatenate([[0], np.cumsum(n_features_list)])\nslices = [\n    slice(start, end)\n    for start, end in zip(start_and_end[:-1], start_and_end[1:])\n]\n\n# Create a different ``Kernelizer`` for each feature space.\nkernelizers = [(\"space %d\" % ii, Kernelizer(), slice_)\n               for ii, slice_ in enumerate(slices)]\ncolumn_kernelizer = ColumnKernelizer(kernelizers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the random-search model\nWe use very few iteration on purpose, to make the random search suboptimal,\nand refine it with hyperparameter gradient descent.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "solver_params = dict(\n    n_iter=5,\n    alphas=np.logspace(-10, 10, 41),\n)\n\nmodel_1 = MultipleKernelRidgeCV(kernels=\"precomputed\", solver=\"random_search\",\n                                solver_params=solver_params, random_state=rng)\npipe_1 = make_pipeline(column_kernelizer, model_1)\n\n# Fit the model on all targets\npipe_1.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the gradient-descent model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "solver_params = dict(\n    max_iter=100,\n    hyper_gradient_method=\"direct\",\n    max_iter_inner_hyper=10,\n    initial_deltas=\"here_will_go_the_previous_deltas\"\n)\n\nmodel_2 = MultipleKernelRidgeCV(kernels=\"precomputed\", solver=\"hyper_gradient\",\n                                solver_params=solver_params)\npipe_2 = make_pipeline(column_kernelizer, model_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use the random-search to initialize the gradient-descent\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We might want to refine only the best predicting targets, since the\n# hyperparameter gradient descent is less efficient over many targets.\ntop = 60  # top 60%\nbest_cv_scores = backend.to_numpy(pipe_1[-1].cv_scores_.max(0))\nmask = best_cv_scores > np.percentile(best_cv_scores, 100 - top)\n\npipe_2[-1].solver_params['initial_deltas'] = pipe_1[-1].deltas_[:, mask]\npipe_2.fit(X_train, Y_train[:, mask])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute predictions on a test set\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\n# use the first model for all targets\ntest_scores_1 = pipe_1.score(X_test, Y_test)\n\n# use the second model for the refined targets\ntest_scores_2 = backend.copy(test_scores_1)\ntest_scores_2[mask] = pipe_2.score(X_test, Y_test[:, mask])\n\ntest_scores_1 = backend.to_numpy(test_scores_1)\ntest_scores_2 = backend.to_numpy(test_scores_2)\nplt.figure(figsize=(4, 4))\nplt.scatter(test_scores_1, test_scores_2, alpha=0.3)\nplt.plot(plt.xlim(), plt.xlim(), color='k', lw=1)\nplt.xlabel(r\"Base model\")\nplt.ylabel(r\"Refined model\")\nplt.title(\"$R^2$ generalization score\")\nplt.tight_layout()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
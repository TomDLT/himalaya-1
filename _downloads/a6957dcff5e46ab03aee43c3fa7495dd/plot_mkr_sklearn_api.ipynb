{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Multiple kernel ridge with scikit-learn API\nThis example demonstrates how to solve multiple kernel ridge regression, using\nscikit-learn API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom himalaya.backend import set_backend\nfrom himalaya.kernel_ridge import KernelRidgeCV\nfrom himalaya.kernel_ridge import MultipleKernelRidgeCV\nfrom himalaya.kernel_ridge import Kernelizer\nfrom himalaya.kernel_ridge import ColumnKernelizer\n\nfrom sklearn.pipeline import make_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, we use the ``torch`` backend.\n\nTorch can perform computations both on CPU and GPU.\nTo use the CPU, use the \"torch\" backend.\nTo use GPU, you can either use the \"torch\" backend and move your data to GPU\nwith the ``.cuda`` method, or you can use the \"torch_cuda\" backend which calls\nthis method in ``backend.asarray``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "backend = set_backend(\"torch_cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate a random dataset\n- Xs_train : list of arrays of shape (n_samples_train, n_features)\n- Xs_test : list of arrays of shape (n_samples_test, n_features)\n- Y_train : array of shape (n_samples_train, n_targets)\n- Y_test : array of shape (n_repeat, n_samples_test, n_targets)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_samples_train = 1000\nn_samples_test = 300\nn_targets = 1000\nn_features_list = [1000, 1000, 500]\nfeature_names = [\"feature space A\", \"feature space B\", \"feature space C\"]\n\nXs_train = [\n    backend.randn(n_samples_train, n_features)\n    for n_features in n_features_list\n]\nXs_test = [\n    backend.randn(n_samples_test, n_features) for n_features in n_features_list\n]\nws = [\n    backend.randn(n_features, n_targets) / n_features\n    for n_features in n_features_list\n]\nY_train = backend.stack([X @ w for X, w in zip(Xs_train, ws)]).sum(0)\nY_test = backend.stack([X @ w for X, w in zip(Xs_test, ws)]).sum(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optional: Add some arbitrary scalings per kernel\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if True:\n    scalings = [0.2, 5, 1]\n    Xs_train = [X * scaling for X, scaling in zip(Xs_train, scalings)]\n    Xs_test = [X * scaling for X, scaling in zip(Xs_test, scalings)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Concatenate the feature spaces and move to GPU with ``backend.asarray``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_train = backend.asarray(backend.concatenate(Xs_train, 1), dtype=\"float32\")\nX_test = backend.asarray(backend.concatenate(Xs_test, 1), dtype=\"float32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We could precompute the kernels by hand on ``Xs_train``, as done in\n``plot_mkr_random_search.py``. Instead, here we use the\n``ColumnKernelizer`` to make a ``scikit-learn`` ``Pipeline``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Find the start and end of each feature space X in Xs\nstart_and_end = np.concatenate([[0], np.cumsum(n_features_list)])\nslices = [\n    slice(start, end)\n    for start, end in zip(start_and_end[:-1], start_and_end[1:])\n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a different ``Kernelizer`` for each feature space. Here we use a\nlinear kernel for all feature spaces, but ``ColumnKernelizer`` accepts any\n``Kernelizer``, or ``scikit-learn`` ``Pipeline`` ending with a\n``Kernelizer``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "kernelizers = [(name, Kernelizer(), slice_)\n               for name, slice_ in zip(feature_names, slices)]\ncolumn_kernelizer = ColumnKernelizer(kernelizers)\n\n# Note that ``ColumnKernelizer`` has a parameter ``n_jobs`` to parallelize each\n# kernelizer, yet such parallelism does not work with GPU arrays."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the model\n\nThe class takes a number of common parameters during initialization, such as\n`kernels` or `solver`. Since the solver parameters might be different\ndepending on the solver, they can be passed in the `solver_params` parameter.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we use the \"random_search\" solver.\nWe can check its specific parameters in the function docstring:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "solver_function = MultipleKernelRidgeCV.ALL_SOLVERS[\"random_search\"]\nprint(\"Docstring of the function %s:\" % solver_function.__name__)\nprint(solver_function.__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use 100 iterations to have a reasonably fast example (~40 sec).\nTo have a better convergence, we probably need more iterations.\nNote that there is currently no stopping criterion in this method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_iter = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Grid of regularization parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "alphas = np.logspace(-10, 10, 41)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Batch parameters are used to reduce the necessary GPU memory. A larger value\nwill be a bit faster, but the solver might crash if it runs out of memory.\nOptimal values depend on the size of your dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_targets_batch = 1000\nn_alphas_batch = 20\nn_targets_batch_refit = 200\n\nsolver_params = dict(n_iter=n_iter, alphas=alphas,\n                     n_targets_batch=n_targets_batch,\n                     n_alphas_batch=n_alphas_batch,\n                     n_targets_batch_refit=n_targets_batch_refit,\n                     jitter_alphas=True)\n\nmodel = MultipleKernelRidgeCV(kernels=\"precomputed\", solver=\"random_search\",\n                              solver_params=solver_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define and fit the pipeline\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipe = make_pipeline(column_kernelizer, model)\npipe.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the convergence curve\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# ``cv_scores`` gives the scores for each sampled kernel weights.\n# The convergence curve is thus the current maximum for each target.\ncv_scores = backend.to_numpy(pipe[1].cv_scores_)\ncurrent_max = np.maximum.accumulate(cv_scores, axis=0)\nmean_current_max = np.mean(current_max, axis=1)\n\nx_array = np.arange(1, len(mean_current_max) + 1)\nplt.plot(x_array, mean_current_max, '-o')\nplt.grid(\"on\")\nplt.xlabel(\"Number of kernel weights sampled\")\nplt.ylabel(\"L2 negative loss (higher is better)\")\nplt.title(\"Convergence curve, averaged over targets\")\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare to ``KernelRidgeCV``\nCompare to a baseline ``KernelRidgeCV`` model with all the concatenated features.\nComparison is performed using the prediction scores on the test set.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit the baseline model ``KernelRidgeCV``\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "baseline = KernelRidgeCV(kernel=\"linear\", alphas=alphas)\nbaseline.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute scores of both models\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores = pipe.score(X_test, Y_test)\nscores = backend.to_numpy(scores)\n\nscores_baseline = baseline.score(X_test, Y_test)\nscores_baseline = backend.to_numpy(scores_baseline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot histograms\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bins = np.linspace(min(scores_baseline.min(), scores.min()),\n                   max(scores_baseline.max(), scores.max()), 50)\nplt.hist(scores, bins, alpha=0.5, label=\"MultipleKernelRidgeCV\")\nplt.hist(scores_baseline, bins, alpha=0.5, label=\"KernelRidgeCV\")\nplt.xlabel(r\"$R^2$ generalization score\")\nplt.title(\"Histogram over targets\")\nplt.legend()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
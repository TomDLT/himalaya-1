{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Multiple kernel ridge regression\nThis example demonstrates how to solve multiple kernel ridge regression.\nIt uses random search and cross validation to select optimal hyperparameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom himalaya.backend import set_backend\nfrom himalaya.kernel_ridge import solve_multiple_kernel_ridge_random_search\nfrom himalaya.kernel_ridge import predict_and_score_weighted_kernel_ridge\nfrom himalaya.scoring import r2_score_split\nfrom himalaya.viz import plot_alphas_diagnostic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, we use the ``cupy`` backend, and fit the model on GPU.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "backend = set_backend(\"cupy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate a random dataset\n\n- Xs_train : list of arrays of shape (n_samples_train, n_features)\n- Xs_test : list of arrays of shape (n_samples_test, n_features)\n- Y_train : array of shape (n_samples_train, n_targets)\n- Y_test : array of shape (n_repeat, n_samples_test, n_targets)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_samples_train = 1000\nn_samples_test = 300\nn_targets = 1000\nn_features_list = [1000, 1000, 500]\n\nXs_train = [\n    backend.randn(n_samples_train, n_features)\n    for n_features in n_features_list\n]\nXs_test = [\n    backend.randn(n_samples_test, n_features) for n_features in n_features_list\n]\nws = [\n    backend.randn(n_features, n_targets) / n_features\n    for n_features in n_features_list\n]\nY_train = backend.stack([X @ w for X, w in zip(Xs_train, ws)]).sum(0)\nY_test = backend.stack([X @ w for X, w in zip(Xs_test, ws)]).sum(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optional: Add some arbitrary scalings per kernel\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if True:\n    scalings = [0.2, 5, 1]\n    Xs_train = [X * scaling for X, scaling in zip(Xs_train, scalings)]\n    Xs_test = [X * scaling for X, scaling in zip(Xs_test, scalings)]\n\nY_train -= Y_train.mean(0)\nY_test -= Y_test.mean(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Precompute the linear kernels\nWe also cast them to float32.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Ks_train = backend.stack([X_train @ X_train.T for X_train in Xs_train])\nKs_train = backend.asarray(Ks_train, dtype=backend.float32)\nY_train = backend.asarray(Y_train, dtype=backend.float32)\n\nKs_test = backend.stack(\n    [X_test @ X_train.T for X_train, X_test in zip(Xs_train, Xs_test)])\nKs_test = backend.asarray(Ks_test, dtype=backend.float32)\nY_test = backend.asarray(Y_test, dtype=backend.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the solver, using random search\nThis method should work fine for\nsmall number of kernels (< 20). The larger the number of kenels, the larger\nwe need to sample the hyperparameter space (i.e. increasing ``n_iter``).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we use 100 iterations to have a reasonably fast example (~40 sec).\nTo have a better convergence, we probably need more iterations.\nNote that there is currently no stopping criterion in this method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_iter = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Grid of regularization parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "alphas = np.logspace(-10, 10, 21)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Batch parameters are used to reduce the necessary GPU memory. A larger value\nwill be a bit faster, but the solver might crash if it runs out of memory.\nOptimal values depend on the size of your dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_targets_batch = 1000\nn_alphas_batch = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If ``return_weights == \"dual\"``, the solver will use more memory.\nTo mitigate this, you can reduce ``n_targets_batch`` in the refit\nusing ```n_targets_batch_refit``.\nIf you don't need the dual weights, use ``return_weights = None``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "return_weights = 'dual'\nn_targets_batch_refit = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the solver. For each iteration, it will:\n\n- sample kernel weights from a Dirichlet distribution\n- fit (n_splits * n_alphas * n_targets) ridge models\n- compute the scores on the validation set of each split\n- average the scores over splits\n- take the maximum over alphas\n- (only if you ask for the ridge weights) refit using the best alphas per\n  target and the entire dataset\n- return for each target the log kernel weights leading to the best CV score\n  (and the best weights if necessary)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = solve_multiple_kernel_ridge_random_search(\n    Ks=Ks_train,\n    Y=Y_train,\n    n_iter=n_iter,\n    alphas=alphas,\n    n_targets_batch=n_targets_batch,\n    return_weights=return_weights,\n    n_alphas_batch=n_alphas_batch,\n    n_targets_batch_refit=n_targets_batch_refit,\n    jitter_alphas=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we used the ``cupy`` backend, the results are ``cupy`` arrays, which are\non GPU. Here, we cast the results back to CPU, and to ``numpy`` arrays.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "deltas = backend.to_numpy(results[0])\ndual_weights = backend.to_numpy(results[1])\ncv_scores = backend.to_numpy(results[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the convergence curve\n\n``cv_scores`` gives the scores for each sampled kernel weights.\nThe convergence curve is thus the current maximum for each target.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "current_max = np.maximum.accumulate(cv_scores, axis=0)\nmean_current_max = np.mean(current_max, axis=1)\nx_array = np.arange(1, len(mean_current_max) + 1)\nplt.plot(x_array, mean_current_max, '-o')\nplt.grid(\"on\")\nplt.xlabel(\"Number of kernel weights sampled\")\nplt.ylabel(\"L2 negative loss (higher is better)\")\nplt.title(\"Convergence curve, averaged over targets\")\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the optimal alphas selected by the solver\n\nThis plot is helpful to refine the alpha grid if the range is too small or\ntoo large.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "best_alphas = 1. / np.sum(np.exp(deltas), axis=0)\nplot_alphas_diagnostic(best_alphas, alphas)\nplt.title(\"Best alphas selected by cross-validation\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute the predictions on the test set\n(requires the dual weights)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "split = False\nscores = predict_and_score_weighted_kernel_ridge(\n    Ks_test, dual_weights, deltas, Y_test, split=split,\n    n_targets_batch=n_targets_batch, score_func=r2_score_split)\nscores = backend.to_numpy(scores)\n\nplt.hist(scores, 50)\nplt.xlabel(r\"$R^2$ generalization score\")\nplt.title(\"Histogram over targets\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute the split predictions on the test set \n(requires the dual weights)\n\nHere we apply the dual weights on each kernel separately\n(``exp(deltas[i]) * kernel[i]``), and we compute the R\\ :sup:`2` scores\n(corrected for correlations) of each prediction.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "split = True\nscores = predict_and_score_weighted_kernel_ridge(\n    Ks_test, dual_weights, deltas, Y_test, split=split,\n    n_targets_batch=n_targets_batch, score_func=r2_score_split)\nscores = backend.to_numpy(scores)\n\nbins = np.linspace(scores.min(), scores.max(), 50)\nfor score in scores:\n    plt.hist(score, bins, alpha=0.5)\nplt.title(r\"Histogram of $R^2$ generalization score split between kernels\")\nplt.legend([\"kernel %d\" % kk for kk in range(scores.shape[0])])\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}